{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f7a90a",
   "metadata": {},
   "source": [
    "## INtroduction of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79db99",
   "metadata": {},
   "source": [
    "1.RNN/LSTM/GRU RNN\n",
    "2.Encoder Decoder Architectore\n",
    "3. Attention Mechanisome\n",
    "4. Transformers\n",
    "\n",
    "1.Why Transformer,\n",
    "2. Architecture of Transformer,\n",
    "3.Self Attention ---> Q,K,V,\n",
    "4. Positional Encoding,\n",
    "5. Multi Head Attention,\n",
    "6 . Cobining the working of Transformer,\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d5788",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "send the word with time stamp but not pass the sentences paralaly \n",
    "Seq 2seq  attention mechanisome not effective work on  the large sentences.\n",
    " \n",
    " ### TRansformers are not user LSTM RNN\n",
    " they use self attention module \n",
    " becode of self attention model we able passing the word parellaly to encoder\n",
    "\n",
    " ### positional encoding\n",
    "TRansformers learning --->multimodel tasks--->NLP+Image\n",
    "\n",
    "TRansformers use Bart and  GPT model for training \n",
    " \n",
    " \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f736",
   "metadata": {},
   "source": [
    "# Contecxtual Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f67a50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "1. Embeding layre used for sentence to fixed vector like word2vect embeding techniques used tradition embeding.\n",
    "\n",
    "\n",
    "2. Sef attention used for relation ship with words of sentence that used to contectual vector embeding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2378526",
   "metadata": {},
   "source": [
    "# Basic Transformer Architecture{Seq2Seq}--->Language translform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01da67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Encoder                                            Decoder\n",
    "                                                         ^\n",
    "                                                         |\n",
    "                                            Feed forword neural network\n",
    "feed forword neural network                           \n",
    "             ^                               Encoder -Decoder Attention\n",
    "             |                 --------->                ^\n",
    "                                                         |\n",
    "       Self Attention                             self Attention\n",
    "             ^\n",
    "             |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "document to pages ,pages to corpus,corpus to sentence ---> to words --->context vector(using embeding layers (or techniques word2vec))\n",
    "\n",
    "\n",
    "\n",
    "# transformedr use muly layers Encoder and Decoder that containe encoder(self attention, feed forword neural network  ) decoder containe techniques\n",
    "(feed forword neural network,encoder -decoder attention,self attention)\n",
    "\n",
    "## all te word sent parellaly that increse effeciency\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040525e2",
   "metadata": {},
   "source": [
    "# SElf Attention Layer Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c168f5e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "self attention also known as scale dot-product attention,that help to importance of deffrent token besed on weights\n",
    "\n",
    "\n",
    "inputs are:query ,keys,and values\n",
    "\n",
    "model-->we have to create a model wich compute the qurey ,key and values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query vector:\n",
    " calculate the attention of the vectore,wich is importance of other token in the context of the current token.\n",
    "\n",
    " Focuse determination:\n",
    "\n",
    " Contectual understanding:\n",
    "\n",
    "\n",
    "Key Vector: key vector represent all the token in the sequence and used to compaire with the query vectore to calculate attention scores.\n",
    "    importance:\n",
    "        Relevance mesurements: keys are compaired with query to mesure the relevance or compatibility of each token withe corrent token .This comparisone helps in detenining the how much attention each token should recive.\n",
    "\n",
    "        Information Retrivals: keys play cretical roalin retrivaing the most relevent information from the sequence of providing a basis for the attention mechanisome to compute similarity Score.\n",
    "\n",
    "\n",
    "Value Cectore:\n",
    "    value vector hold the actual information that will be aggregated to from the output of the attention mechanisome .\n",
    "\n",
    "    importance:\n",
    "      information aggregated\n",
    "      context preservation\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "1.Token Embeding used tradition embeding techniques like word2vec one hot encoding,bag of word ,continuse bag of word\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.Lenear Transformation:\n",
    "   We convert Q,K,V by helping the Embeding by Learned the weights matrics,weight-Q,Weight-K,Weights-V\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WQ=WK=WV=I(Identity matrix)\n",
    "eg.the cat sat \n",
    "\n",
    "Qthe=Kthe=Vthe=[1010]\n",
    "Qcat=Kcat=Vcat=[0101]\n",
    "Qsat=Ksat=Vsat=[1111]\n",
    "\n",
    "\n",
    "3. Compute the attention Score:\n",
    "\n",
    "The-comput that word score:(Using transfor opration)\n",
    "    Score(Qthe,Kthe)=[1010].[1010]T=2\n",
    "    Score(Qthe,Kcat)=[1010].[0101]T=0\n",
    "    Score(Qthe,Ksat)=[1010].[1111]T=2\n",
    "\n",
    "For Cat word:\n",
    "    Score(Qcat,Ksat)=[0101].[1010]T=0\n",
    "    Score(Qcat,Kcat)=[0101].[0101]T=2\n",
    "    Score(Qcat,Ksat)=[0101].[1111]T=2\n",
    "\n",
    "For Sat word:\n",
    "    Score(Qsat,Kthe)=[1111].[1010]T=2\n",
    "    Score(Qsat,Kcat)=[1111].[0101]T=2\n",
    "    Score(Qsat,Ksat)=[1111].[1111]T=4\n",
    "\n",
    "\n",
    "\n",
    "4.Scalling:\n",
    "     we take the attention score and scale down by deviding the score by the squre root of dimention of key vector\n",
    "     dk=4  squre root=2\n",
    "      scaled_score= (Qthe,Kthe)=2/2=1\n",
    "      scaled_score= (Qthe,Kcat)=0/2=0\n",
    "      scaled_score= (Qthe,Ksat)=2/2=1\n",
    "\n",
    "      scaled_score= (Qcat,Kthe)=0/2=0\n",
    "      scaled_score= (Qcat,Kcat)=2/2=1\n",
    "      scaled_score= (Qcat,Ksat)=2/2=1\n",
    "\n",
    "\n",
    "      scaled_score= (Qsat,Kthe)=2/2=1\n",
    "      scaled_score= (Qsat,Kcat)=2/2=1\n",
    "      scaled_score= (Qsat,Ksat)=4/2=2\n",
    "\n",
    "     prevent the dot product fromgrowing to large.\n",
    "\n",
    "\n",
    "     1.Gradient Exploding\n",
    "     2.Softmax saturation\n",
    "\n",
    "\n",
    "5.apply soft max over the sacling result:\n",
    "  attention Weights of(\"The\") = Softmax([101])=[0.1523,0.422,0.3223]\n",
    "  attention Weights of(\"cat\") = Softmax([011])=\n",
    "  attention Weights of(\"sat\") = Softmax([112])=\n",
    "\n",
    "\n",
    "\n",
    "6 Weight sum of Value:\n",
    "\n",
    "   we multiply attention weights by corespoding value vector\n",
    "   for the token the=output(\"the\")=0.4223*Vthe+0.1554*Vcat+0.4223*Vsat\n",
    "                                  =0.4223*[1010]+0.1554*[0101]+0.4223*[1111]\n",
    "                                  =[0.4223,0,0.4223,0]+[0,0.1553,0,0.1554]+[0.4223,0,0,0.4223]\n",
    "                                  =[Softmax*V]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378e930",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ceed89",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef29ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Advantages\n",
    "   word token it can process parellaly in TRansformer\n",
    "disadvantage:\n",
    "    lack of sequential structure of words\n",
    "\n",
    "position encodeing used to tracke the sequential structure of word that used positional encoding vector \n",
    "\n",
    "\n",
    "Typers fo positional Encoding:\n",
    "   1.sinsusoidal positional encoding\n",
    "\n",
    "       It used sign and cose funtion of diffrent to create positional encoding\n",
    "\n",
    "       formulas:\n",
    "             Where pos is position,i is the dimentions,dmodel is the dimentionality of the embedding.\n",
    "\n",
    "\n",
    "         positional encoding(pos,2i)=sin(pos/10000^2i/dmodel)\n",
    "         positional encoding(pos,2i+1)=cos(pos/10000^2i/dmodel)\n",
    "      \n",
    "         eg.the cat set \n",
    "         the-->[0.1 0.2 0.3 0.4]\n",
    "         cat-->[0.5 0.6 0.7 0.8]\n",
    "         sat-->[0.9 1.0 1.1 1.2]\n",
    "         use positional encoding with sinsusoidal positional encodeing\n",
    "           \n",
    "        positional encoding(pos,2i)=sin(pos/10000^2i/dmodel)\n",
    "         positional encoding(pos,2i+1)=cos(pos/10000^2i/dmodel)\n",
    "\n",
    "         for our example dimention on encoding is 4:\n",
    "\n",
    "         for position =0\n",
    "         \n",
    "         positional encoding(0,0)=sin(0/10000^2 0/4)=sin(0)=0\n",
    "         positional encoding(0,1)=cos(0/10000^2 1/4) =cos(0)=1\n",
    "         positional encoding(0,2)=sin(0/10000^2 2/4)=sin(0)=0\n",
    "         positional encoding(0,3)=cos(0/10000^2 1/4) =cos(0)=1\n",
    "\n",
    "          for position =1\n",
    "          positional encoding(1,0)=sin(0/10000^2 0/4)=sin(0)=0.84\n",
    "         positional encoding(1,1)=cos(0/10000^2 2/4) =cos(0)=0.564\n",
    "         positional encoding(1,2)=sin(0/10000^2 2/4)=sin(0)=0.01\n",
    "         positional encoding(1,3)=cos(0/10000^2 1/4) =cos(0)=0.9995\n",
    "\n",
    "                 encoding vector                 positional encoding\n",
    "          The---->[0.1 0.2 0.3 0.4]------------------>[0 1 0 1]\n",
    "           cat--->[0.5 0.6 0.7 0.8]----------------->[0.84 0.54 0.01 0.99]\n",
    "           sat--->[0.9 1.0 1.1 1.2]----------------->[0.4 0.34 0.23 0.53]\n",
    "\n",
    "\n",
    "                                             |          |\n",
    "                                         self max attention\n",
    "                                                  |\n",
    "                                                  |\n",
    "             [0 1 0 1]                    [0.84 0.54 0.01 0.99]            [0.4 0.34 0.23 0.53]\n",
    "                  +                                  +                         +\n",
    "            [0.1 0.2 0.3 0.4]                [0.5 0.6 0.7 0.8]              [0.9 1.0 1.1 1.2]                \n",
    "\n",
    "   2.learned positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b1435",
   "metadata": {},
   "source": [
    "# Nornalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556fab5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "1.Bach Normalization (ann,rnn used that normalization)\n",
    "     calculate mean for batch,and standerd deviation  apply zscore\n",
    "       zscore=x1-mean/standerd deviation\n",
    "2.Layer Normalization(transformer used leayerd normalization)\n",
    "      it used to execute layer by leayerd \n",
    "\n",
    "gama and bita are lernable paramitter (Scale and shift paramitter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f182b0f",
   "metadata": {},
   "source": [
    "Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af4e2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "1.cat =[2.0 4.0 6.0 8.0]\n",
    "2.paramitter=gama=[1.0,1.0,1.0,1.0]-->learned Scale\n",
    "             bita=[0.0,0.0,0.0,0.0]-->Shift\n",
    "\n",
    "\n",
    "\n",
    "            zscore=Xi-min/standerd deviation\n",
    "\n",
    "            i> Compute the mean:\n",
    "                     min=1/4(2.0+4.0+6.0+8.0)\n",
    "                     =20.0/4=5.0\n",
    "            ii> Compute the verience(standerd deviation^2)\n",
    "                 standerd deviation^2=1/4[(2.0-5.0)^2+(4.0-5.0)^2+(6.0-5.0)^2+(8.0-5.0)^2]=5.0\n",
    "            iii> Normalize the inputs:\n",
    "                  X^1=Xi-min/sqrt (standerd deviation + Epcilon)\n",
    "                  E(Epcilon)=1e^-5 use to avoide devision by zero\n",
    "\n",
    "                  min/sqrt (standerd deviation + Epcilon)=sqrt(5.0 +1e^-5) =sqrt(5.0001)=2.236\n",
    "\n",
    "                  X1=2.0-5.0/2.236=-1.34\n",
    "                  x2=4.0-5.0/2.236=-0.45\n",
    "                  x3=6.0-5.0/2.236=0.45\n",
    "                  x4=8.0-5.0/2.236=1.34\n",
    "\n",
    "\n",
    "                  Normalize vector:\n",
    "                    x^=[-1.34,-0.45,0.45,1.34]\n",
    "\n",
    "\n",
    "    4.scale and shift:\n",
    "         Yi=gama i * Xi + Beta i\n",
    "                                gama=[1.0,1.0,1.0,1.0]-->learned Scale\n",
    "                                 bita=[0.0,0.0,0.0,0.0]-->Shift\n",
    "         y=[-1.34,-0.45,0.45,1.34]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d71ba",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b1e2d",
   "metadata": {},
   "source": [
    "## Masked multi head self attention\n",
    "## multi head self attention(Encoder Decoder attention)\n",
    "## Feed Forword Nural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949190b5",
   "metadata": {},
   "source": [
    "### Masked multi head self attention\n",
    " ##### Input embedding and positional embeding\n",
    " ##### Linear Projection for Q V K\n",
    " ##### Scalled dot product attention\n",
    " ##### Mask appliction\n",
    "       Look Ahead mask\n",
    "       Padding Mask\n",
    " ##### Multi head attention\n",
    " ##### concatination and final linear  Normalization\n",
    " ##### Residual Connection and layer Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618a80a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "1. Input Embedding and positional Embedding:\n",
    "output Embedding:\n",
    "     [ [0.1 0.2 0.3 0.4]              \n",
    "       [0.5 0.6 0.7 0.8]    \n",
    "       [0.9 1.0 1.1 1.2]   \n",
    "       [0.0 0.0 0.0 0.0]]\n",
    "\n",
    "       positional Embedding is 0\n",
    "\n",
    "\n",
    "2. Linear Projection for Q,K,V\n",
    "   WQ=WK=WV=I(is known as identical matrix)\n",
    "   Create Query(Q),Kry(K),Value(V) Vector \n",
    "\n",
    "\n",
    "   Q=output Embeding * WQ=output Embedding\n",
    "   K=output Embeding * Wk=output Embedding\n",
    "   V=output Embeding * WV=output Embedding\n",
    "                                   K T(key transpose) that have the multiple key transpose\n",
    "   Q=K=V= [ [0.1 0.2 0.3 0.4]     [0.1]    [0.5]        \n",
    "          [0.5 0.6 0.7 0.8]   *   [0.2]    [0.6]\n",
    "          [0.9 1.0 1.1 1.2]       [0.3]    [0.7]\n",
    "          [0.0 0.0 0.0 0.0]]      [0.4]    [0.8]\n",
    "3. Scal dot Product  attention calculation\n",
    "   Scores=Q*K T/Sqrt(dk)\n",
    "         =Q*K T/2          \n",
    "\n",
    "                       [[0.1*0.1+0.2*0.2+0.3*0.3+0.4*0.4,\n",
    "                        0.1*0.5+0.2*0.6+0.3*0.7+0.4*0.8\n",
    "                       ]\n",
    "                       [0.5*0.1+0.6*0.2+0.7*0.3+0.8*0.4,\n",
    "                        0.5*0.5+0.6*0.6+0.7*0.7+0.8*0.8\n",
    "                       ]\n",
    "                       [0.9*0.1+1.0*0.2+1.1*0.3+1.2*0.4,\n",
    "                        0.9*0.5+1.0*0.6+1.1*0.7+1.2*0.8\n",
    "                       ]\n",
    "                       [0.0*0.1+0.0*0.2+0.0*0.3+0.0*0.4,\n",
    "                        0.0*0.5+0.0*0.6+0.0*0.7+0.0*0.8\n",
    "                       ]]\n",
    "\n",
    "\n",
    "   score:[[0.3,0.7]      \n",
    "          [0.7,1.9]\n",
    "          [1.1 ,3.1]\n",
    "          [0.0,0.0]]\n",
    "\n",
    "\n",
    "4.Masked application:\n",
    "    it help to managing the structure of the sequence being processed and ensure the model behave correctly during training and inferance.\n",
    "\n",
    "    purpose:\n",
    "     1.To handal sequence of diffrent length in batch\n",
    "     2. To ensure that padding token witch are added to make sequnces of uniform length do not affect the model prediction.\n",
    "\n",
    "\n",
    "   A padding  mask The token and ignored\n",
    "\n",
    "   E.g:   seq_inp1=[1,2,3]\n",
    "          seq_inp2=[4,5,0] 0 is the padding token\n",
    "                           0 is inferance the attention mechanisome-->lead to incorrect or biased prediction\n",
    "                           e.g(if padding added 98 0 and value are 2 that not get efficient result)\n",
    "\n",
    "\n",
    "                                      eg. padding mask  [1 1 1]\n",
    "                                                        [1 1 0] 0 is padding mask  , 0 is define padding is added in sequnnce for making same size input\n",
    "            padding mask\n",
    "   masking\n",
    "            Look ahed mask     \n",
    "\n",
    "\n",
    "   Look ahed Mask:\n",
    "                 maintain auto regrassive property\n",
    "\n",
    "    to ensure that eatch position in the decoder\n",
    "    1.output sequence can only attend to the previous posiition but not future position\n",
    "    2. seq2seq model like laguage translate ,model\n",
    "\n",
    "    e.g: [4,5,0]--->[1,1,0] convert 1D to 2D mask\n",
    "                    [1 1 0]    for eatch tokkent in the sequence ,the mask should indicate witch token it can attend to\n",
    "                    [1 1 0] \n",
    "                    [0 0 0]\n",
    "\n",
    "\n",
    "\n",
    "   * Look Ahed mask:\n",
    "         [1 0 0]\n",
    "         [1 1 0]\n",
    "         [1 1 1]\n",
    "     \n",
    "\n",
    "   * combine padding and Looking ahed mask\n",
    "          Element wise multiplication of 2 mask \n",
    "\n",
    "      combine mask=[[1 0 0]\n",
    "                   [1 1 0]\n",
    "                   [0 0 0]] \n",
    "\n",
    "\n",
    "   score:                      Look Ahed mask:\n",
    "          [[0.3,0.7]                        [[1 0]\n",
    "          [0.7,1.9]                          [1 1]\n",
    "          [1.1 ,3.1]         *               [1 1]\n",
    "          [0.0,0.0]]                         [1 1]]\n",
    "\n",
    "\n",
    "\n",
    " padding Masking [Extended to 2D format]\n",
    " [[1,0]\n",
    "  [1,1]\n",
    "  [1,1]\n",
    "  [0,0]]\n",
    "\n",
    "  combine Msk: look ahed mask  * Padding Mask\n",
    "        [[1*1 ,0*0]          [[1 ,0]      [[1 , - infinity]\n",
    "        [1*1,1*1]     =       [1 ,1]   =  [1 ,1]\n",
    "        [1*1 ,1*1]            [1 ,1]      [1 ,1]\n",
    "        [0*1,0*1]]            [0 ,0]       [- infinity ,- infinity]]\n",
    "\n",
    "Masked Score:\n",
    "[0.3 ,- infinity]\n",
    "[0.7 ,1.9]                                   \n",
    "[1.1 ,3.1]\n",
    "[0.0 , - infinity]\n",
    "\n",
    "Zero out the infulence witch the softmax is applide\n",
    "\n",
    "\n",
    "* softmax\n",
    "   Score=softmax(masked Scores)\n",
    "\n",
    "\n",
    "* self attention Weight sum of value:\n",
    "      attention output=Softmax Score* Vector\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f366c",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a6143",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "1.encoder output --> set of action vector Key and Value\n",
    "2. Masked multiheead attention-->attention vector Q Query\n",
    "this are to used by each decoder in its \"encoder - decoder \" attention layer\n",
    "that helps the decoder to focuse on appropriate places in the input sequence.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
